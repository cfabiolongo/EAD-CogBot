# Natural Language to First Order Logic
[NL_TO_FOL]
# enable verbose conversion
VERBOSE = false
# WordNet language
LANGUAGE = eng
# Words for asserting logic implications (WHEN, IF, WHILE, etc.)
CONDITIONAL_WORDS = WHEN
# Enable assignment rules creation
ASSIGN_RULES_ADMITTED = false
# Deadjectival nominalization activation (functionally subsumed by GEN_ADJ = true)
DEAJECT_NOM = false
# Deverbal nominalization activation (work in progress)
DEVERB_NOM = false
# Lemmatization activation
LEMMATIZATION = false

# Lower Knowledge Base
[LKB]
LKB_USAGE = false
HOST = localhost:27017
USER = root
PASSWORD = example
MIN_CONFIDENCE = 0.6
EMPTY_HKB_AFTER_REASONING = no

[AGENT]
# Waiting seconds before returning to idle state
WAIT_TIME = 10
# Telegram token
TELEGRAM_TOKEN = XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
# Operations logging
LOG_ACTIVE = True
# External file for feeding KBs
FILE_KB_NAME = kbs/west25.txt
# Excel file for Low Belief KB exporting
FILE_EXPORT_LKB_NAME = kbs/lkb2_gnd.xlsx
# Inf true, all clauses in Excel file will be grounded
FILE_EXPORT_GND_TERMS = true

[REASONING]
# Enable occur_check inside Unify - to be used carefully preferably with unique labels
OCCUR_CHECK = true
NESTED_REASONING = false

# Selective inclusion/exclusion of Part-of-Speech
[POS]
INCLUDE_ACT_POS = true
INCLUDE_NOUNS_POS = true
INCLUDE_ADJ_POS = true
INCLUDE_PRP_POS = true
INCLUDE_ADV_POS = true
# Object adjective into noun correction (WARNING 1: false only with GEN_ADJ = false)
OBJ_JJ_TO_NOUN = true

# Selective inclusion/exclusion of mods categories on generalizations assertion
[GEN]
GEN_PREP = false
# functionally subsumed by DEAJECT_NOM = true)
GEN_ADJ = false
GEN_ADV = false
GEN_EXTRA = false
# list separate by commas
EXTRA_GEN_POS = VBN

# Common meaning for same lemmas in a session-context (DISAMBIGUATION=ACTIVE)
[GROUNDED_MEANING_CONTEXT]
GMC_ACTIVE = true
GMC_POS = NN, NNS, VBZ, VBP, VB, VBD, JJ

# Question Answering
# LOC_PREPS: in case of where-question, they are all prepositions of possibile candidates (possible responses to the question) will make usage
# TIME_PREPS: in case of when-question, they are all prepositions of possibile candidates (possible responses to the question) will make usage
# COP_VERB: copula is-a for which (Obj1 is-a Obj2) => (Obj2 is-a Obj1)
# ROOT_TENSE_DEBT: all question ROOT tenses that after Question->Answer translations must be present also in the assertions
[QA]
LOC_PREPS = in, at
TIME_PREPS = in, on
COP_VERB = is, was, were
ROOT_TENSE_DEBT = did:VBD, does:VBZ
SHOW_REL = false


[LLM]
# LLM Interaction mode: AD (Abduction-Deduction with FOL-to_NL response), LLM (only Query/Answer LLM), DUAL (AD+LLM)
MODE = AD
# Fol-to_NL Temperature
TEMP_FOL = 0.1
# Question Answering Temperature
TEMP_QA = 0.6
# Max new token number generation
MAX_NEW_TOKENS = 512
# Combination type for Multiple LoRA Adapters: cat (concatenation), linear (linear combination), svd (singular value decomposition)
COMB_TYPE = linear
# Combination weights (FOL, QA)
WEIGHTS = 0.5, 0.1
# Base model
BASE_MODEL = /home/fabio/llama/models/7B-chat
# Adapters names
ADAPTER_NAME1 = llama-fol_50ep
ADAPTER_NAME2 = llama-dolly_qa_100ep
# Adapters path
ADAPTER_PATH1 = /home/fabio/llama/models/finetuned/llama-fol2_50ep
ADAPTER_PATH2 = /home/fabio/llama/models/finetuned/llama-dolly_qa_100ep
# Prefix text for non-hot topic LLM response
PREFIX_LLM_RESP = Well...I'm not sure, but....



